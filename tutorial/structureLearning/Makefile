CP=.:${PROPPR}/bin:${PROPPR}/conf:${PROPPR}/lib/*

### Paths ###

WD=$(shell pwd)

# DIR: for .cfacts, .examples
ifeq ($(strip $(DIR)),)
DIR=.
endif

# NAME: filename stem
ifeq ($(strip $(NAME)),)
NAME=tmp
endif
ifeq ($(strip $(NAME)),uml_1)
JOPTS=-Xmx30G
endif

# SL: Structure learning ruleset
ifeq ($(strip $(SL)),)
SL=$(WD)/h22.ppr
endif

# INTERP: Standalone ruleset
ifeq ($(strip $(INTERP)),)
INTERP=$(WD)/interp.ppr
endif

TRAIN=${NAME}-train
TEST=${NAME}-test

### Settings ##

ifeq ($(strip $(MU)),)
MU=0.0001
endif
ifeq ($(strip $(EPSILON)),)
EPSILON=.0001
endif
ifeq ($(strip $(ALPHA)),)
ALPHA=.001
endif
THREADS=3

# for measuring gradient
GSRW=l2p:mu=$(MU):eta=5.0
# for training final model
TSRW=l2p:mu=$(MU):eta=1.0
EPOCHS=100
PROVER=dpr
APR=eps=$(EPSILON):alph=$(ALPHA)
WEIGHTINGSCHEME=--squashingFunction tanh
UNNORMALIZED=--unnormalized
ifneq ($(strip $(UNNORMALIZED)),)
UNNORMALIZED_LOPT=$(UNNORMALIZED) 1
endif

ifeq ($(strip $(ITERS)),)
ITERS=10
endif

vpath %.examples $(DIR)
vpath %.cfacts $(DIR)

### TARGETS
# To run fully, do
#     $ make isg
#     $ make

all:
	make -e isg &&\
	make -e results.txt

clean:
	rm -f *results.txt *.grounded *.solutions.txt params.*.wts *.gradient \
	$(foreach ruletype,SL delta alone,$(addprefix *_$(ruletype)_*,.ppr .wam))

.PRECIOUS: %.examples params.wts %.solutions.txt

## Below adapted from 
## /remote/curtis/yww/data/forKatie/uml/Makefile
## and
## /remote/curtis/yww/data/forKatie/uml/bin/iterativeGradientFinder.py
## March 2015

# This project iteratively refines a program based on h22.ppr. At each
# iteration (XX), the rulefile ${NAME}_SL_XX.ppr is formed by
# concatenating h22.ppr and those rules generated by the previous
# iteration.
#
# At iteration t, we perform (t-1) epochs of training, then take the
# gradient of the loss. Additional rules are generated based on
# features with a negative gradient. These additional rules are stored
# in ${NAME}_delta_$t.ppr, and we proceed until no new rules are added.
#
# We measure the performance after each iteration by evaluating
# ${NAME}_alone_$t.ppr -- which is formed only from
# ${NAME}_delta_$t.ppr and interp.ppr, and does not include the h22
# rules -- on the test queries.

# Iteration 0 is the baseline, without any additional rules.
#
# Iteration 1 is the SG method, which uses only one gradient
# calculation to compute rules.
#
# All other iterations form the ISG method, which iterates on the
# gradient until no new rules are added. The performance of ISG is the
# performance of the final iteration (aka fig).

structureLearning: $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.solutions.txt,$(subst _delta,-test.alone,$(it))),pre. post.))

yww.pre:       $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.yww,$(subst _delta,-test.alone,$(it))),pre.))

yww.post:       $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.yww,$(subst _delta,-test.alone,$(it))),post.))

yww: yww.pre yww.post

results.txt:       $(foreach it,$(wildcard ${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.results.txt,$(subst _delta,-test.alone,$(it))),pre. post.))
	echo phase.subset.iteration uR mR uMRR mMRR uMAP mMAP > $@
	cat $^ >> $@

# (duplicate yww's evaluation method)
# make  e.g. pre.uml_1-test.alone_01.orderAnswers
# from  e.g. pre.uml_1-test.alone_01.solutions.txt
# using e.g.     uml_1-test.examples
%.orderAnswers: %.solutions.txt
	ROOT=$*;\
	ROOT=$${ROOT#*.};\
	ROOT=$(DIR)/$${ROOT%.*};\
	proppr eval $${ROOT}.examples $< --metric recall --echo |\
	grep "^[0-9]" |\
	awk 'BEGIN{FS=OFS="\t"}{flag=0; if ($$4=="True" || ($$4=="+=" && $$5=="True")) { flag=-1 } print $$2,flag,$$3}' > $@ ;\
	sed 's/\S*\t//;s/\t/\n/g' $${ROOT}.examples |\
	grep "+" |\
	sed 's/+//' |\
	awk 'BEGIN{OFS="\t"}{print "0.0","+1",$$1}' >> $@
	sort -k 3 -k 1rg $@ |\
	awk '{if ($$3!=last) {last=$$3; print}}' > $@.tmp
	mv $@.tmp $@

%.yww: %.orderAnswers
	sort -gr $< | python scripts/avgprec.py | tee $@

# make  e.g. pre.uml_1-test.alone_01.results.txt
# from  e.g. pre.uml_1-test.alone_01.solutions.txt
# using e.g.     uml_1-test.examples
%.results.txt: %.solutions.txt
	STEM=$*;\
	STEM=$${STEM#${DIR}/};\
	ROOT=$${STEM#*.};\
	ROOT=${DIR}/$${ROOT%.*};\
	proppr eval $${ROOT}.examples $< --metric recall --metric mrr --metric map |\
	grep -e "micro:" -e "macro:" |\
	awk '{print $$3}' |\
	tr "\n" " " |\
	awk -v name=$${STEM} '{gsub("\."," ",name); print name,$$0}' > $@
	cat $@

## The common.in target definitions are insufficient for our needs,
## since the program will differ for each iteration. Thus we define
## our own solutions targets, which include the .wam program
## dependency for that iteration.

# make e.g. pre.uml_1-test.SL_01.solutions.txt
# from e.g.     uml_1-test.examples
#               uml_1_SL_01.wam
pre.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam ${NAME}-test.cfacts
	proppr set --programFiles $(word 3,$^):$(word 2,$^)
	PROPPR_JAVA_ARGS=${JOPTS}; \
	proppr answer $< $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED_LOPT} ${WEIGHTINGSCHEME}

# make e.g. post.uml_1-test.SL_01.solutions.txt
# from e.g.      uml_1-test.examples
#                uml_1_SL_01.wam
#               params.SL_01.wts
post.${NAME}-test.%.solutions.txt: ${NAME}-test.examples ${NAME}_%.wam params.%.wts ${NAME}-test.cfacts
	proppr set --programFiles $(word 4,$^):$(word 2,$^)
	PROPPR_JAVA_ARGS=${JOPTS}; \
	proppr answer $< $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED_LOPT} ${WEIGHTINGSCHEME} --params $(word 3,$^)

# make e.g. pre.uml_1-train.SL_01.solutions.txt
# from e.g.     uml_1-train.examples
#               uml_1_SL_01.wam
pre.${NAME}-train.%.solutions.txt: ${NAME}-train.examples ${NAME}_%.wam ${NAME}-train.cfacts
	proppr set --programFiles $(word 3,$^):$(word 2,$^)
	PROPPR_JAVA_ARGS=${JOPTS}; \
	proppr answer $< $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED_LOPT} ${WEIGHTINGSCHEME}

# make e.g. post.uml_1-train.SL_01.solutions.txt
# from e.g.      uml_1-train.examples
#                uml_1_SL_01.wam
#               params.SL_01.wts
post.${NAME}-train.%.solutions.txt: ${NAME}-train.examples ${NAME}_%.wam params.%.wts ${NAME}-train.cfacts
	proppr set --programFiles $(word 4,$^):$(word 2,$^)
	PROPPR_JAVA_ARGS=${JOPTS}; \
	proppr answer $< $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED_LOPT} ${WEIGHTINGSCHEME} --params $(word 3,$^)


# make e.g.      params.alone_01.wts
# from e.g. uml_1-train.alone_01.examples.grounded
params.%.wts: ${TRAIN}.%.examples.grounded
	proppr train $< $@ --threads ${THREADS} --srw ${TSRW} --epochs ${EPOCHS} --apr ${APR} ${WEIGHTINGSCHEME} --traceLosses
	sed -i '/train.cfacts/ {p;s/train/test/};' $@

# ^^^^ sed cmd duplicates the train.cfacts feature and exchanges the name for test.cfacts

# make e.g. uml_1-train.SL_01.examples.grounded
# from e.g. uml_1-train.examples
#           uml_1_SL_01.wam
${NAME}-train.%.examples.grounded: ${NAME}-train.examples ${NAME}_%.wam ${NAME}-train.cfacts
	proppr set --programFiles $(word 3,$^):$(word 2,$^)
	PROPPR_JAVA_ARGS=${JOPTS}; \
	proppr ground $< $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${WEIGHTINGSCHEME}

## Additional targets for building an executable program from gradient-generated rules:

# uml_1_delta_01.ppr + h22.ppr = uml_1_SL_01.ppr
${NAME}_SL_%.ppr: ${NAME}_delta_%.ppr ${SL}
	cat $^ > $@

# uml_1_delta_01.ppr + interp.ppr = uml_1_alone_01.ppr
${NAME}_alone_%.ppr: ${NAME}_delta_%.ppr ${INTERP}
	cat $^ > $@

# Actually run the iterated structural gradient procedure, and supporting target:

isg:
	python scripts/iterativeGradientFinder.py ${NAME} ${ITERS}

# make e.g.           uml_1_01.gradient
# from e.g. uml_1-train.SL_01.examples.grounded
# first training for e.g.   01 epochs
${NAME}_%.gradient: ${NAME}-train.SL_%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.GradientFinder --grounded $< \
	--threads ${THREADS} --apr ${APR} --epochs $* --srw ${GSRW} --gradient $@ \
	--trainer streaming --squashingFunction ReLU

%.wam: %.ppr
	proppr compile $<

.SECONDARY:
