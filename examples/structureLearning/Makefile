include ../Makefile.in

### DATASET
ifeq ($(strip $(DIR)),)
DIR=.
endif
ifeq ($(strip $(NAME)),)
NAME=uml_1
ifeq ($(strip $(JOPTS)),-Xmx10G)
JOPTS=-Xmx30G
endif
endif
ifeq ($(strip $(SL)),)
SL=h22.ppr
endif
ifeq ($(strip $(INTERP)),)
INTERP=interp.ppr
endif

TRAIN=${NAME}-train
TEST=${NAME}-test

# for measuring gradient
GSRW=l2p:mu=0.0001:eta=5.0
# for training final model
TSRW=l2p:mu=0.0001:eta=1.0
EPOCHS=100
PROVER=dpr
APR=eps=0.0001:alph=0.001
WEIGHTINGSCHEME=--weightingScheme tanh
UNNORMALIZED=--unnormalized $(WEIGHTINGSCHEME)

ifeq ($(strip $(ITERS)),)
ITERS=10
endif


### TARGETS
# To run fully, do
#     $ make isg
#     $ make

all: results.txt

include ../common.in

clean:
	rm -f *results.txt *.grounded *.solutions.txt params.*.wts *.gradient \
	$(foreach ruletype,SL delta alone,$(addprefix *_$(ruletype)_*,.ppr .wam))

.PRECIOUS: %.examples params.wts %.solutions.txt

## Below adapted from 
## /remote/curtis/yww/data/forKatie/uml/Makefile
## and
## /remote/curtis/yww/data/forKatie/uml/bin/iterativeGradientFinder.py
## March 2015

# This project iteratively refines a program based on h22.ppr. At each
# iteration (XX), the rulefile ${NAME}_SL_XX.ppr is formed by
# concatenating h22.ppr and those rules generated by the previous
# iteration.
#
# At iteration t, we perform (t-1) epochs of training, then take the
# gradient of the loss. Additional rules are generated based on
# features with a negative gradient. These additional rules are stored
# in ${NAME}_delta_$t.ppr, and we proceed until no new rules are added.
#
# We measure the performance after each iteration by evaluating
# ${NAME}_alone_$t.ppr -- which is formed only from
# ${NAME}_delta_$t.ppr and interp.ppr, and does not include the h22
# rules -- on the test queries.

# Iteration 0 is the baseline, without any additional rules.
#
# Iteration 1 is the SG method, which uses only one gradient
# calculation to compute rules.
#
# All other iterations form the ISG method, which iterates on the
# gradient until no new rules are added. The performance of ISG is the
# performance of the final iteration (aka fig).

structureLearning: $(foreach it,$(wildcard ${DIR}/${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.solutions.txt,$(subst _delta,-test.alone,$(subst $(DIR)/,,$(it)))),pre. post.))

yww.pre:       $(foreach it,$(wildcard ${DIR}/${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.yww,$(subst _delta,-test.alone,$(subst $(DIR)/,,$(it)))),pre.))

yww.post:       $(foreach it,$(wildcard ${DIR}/${NAME}_delta_*.ppr),$(addsuffix $(subst .ppr,.yww,$(subst _delta,-test.alone,$(subst $(DIR)/,,$(it)))),post.))

yww: yww.pre yww.post

results.txt:       $(foreach it,$(wildcard ${DIR}/${NAME}_delta_*.ppr),$(addprefix $(DIR)/,$(addsuffix $(subst .ppr,.results.txt,$(subst _delta,-test.alone,$(subst $(DIR)/,,$(it)))),pre. post.)))
	echo phase.subset.iteration uR mR uMRR mMRR uMAP mMAP > $@
	cat $^ >> $@

# (duplicate yww's evaluation method)
# make  e.g. pre.uml_1-test.alone_01.orderAnswers
# from  e.g. pre.uml_1-test.alone_01.solutions.txt
# using e.g.     uml_1-test.examples
%.orderAnswers: %.solutions.txt
	ROOT=$*;\
	ROOT=$${ROOT#*.};\
	ROOT=$(DIR)/$${ROOT%.*};\
	python ${PROPPR}/scripts/answermetrics.py --data $${ROOT}.examples --answers $< --metric recall --echo |\
	grep "^[0-9]" |\
	awk 'BEGIN{FS=OFS="\t"}{flag=0; if ($$4=="True" || ($$4=="+=" && $$5=="True")) { flag=-1 } print $$2,flag,$$3}' > $@ ;\
	sed 's/\S*\t//;s/\t/\n/g' $${ROOT}.examples |\
	grep "+" |\
	sed 's/+//' |\
	awk 'BEGIN{OFS="\t"}{print "0.0","+1",$$1}' >> $@
	sort -k 3 -k 1rg $@ |\
	awk '{if ($$3!=last) {last=$$3; print}}' > $@.tmp
	mv $@.tmp $@

%.yww: %.orderAnswers
	sort -gr $< | python scripts/avgprec.py | tee $@

# make  e.g. pre.uml_1-test.alone_01.results.txt
# from  e.g. pre.uml_1-test.alone_01.solutions.txt
# using e.g.     uml_1-test.examples
%.results.txt: %.solutions.txt
	STEM=$*;\
	STEM=$${STEM#${DIR}/};\
	ROOT=$${STEM#*.};\
	ROOT=${DIR}/$${ROOT%.*};\
	python ${PROPPR}/scripts/answermetrics.py --data $${ROOT}.examples --answers $< --metric recall --metric mrr --metric map |\
	grep -e "micro:" -e "macro:" |\
	awk '{print $$3}' |\
	tr "\n" " " |\
	awk -v name=$${STEM} '{gsub("\."," ",name); print name,$$0}' > $@
	cat $@

## The common.in target definitions are insufficient for our needs,
## since the program will differ for each iteration. Thus we define
## our own solutions targets, which include the .wam program
## dependency for that iteration.

# make e.g. pre.uml_1-test.SL_01.solutions.txt
# from e.g.     uml_1-test.examples
#               uml_1_SL_01.wam
${DIR}/pre.${NAME}-test.%.solutions.txt: ${DIR}/${NAME}-test.examples ${DIR}/${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${DIR}/${NAME}-test.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED}

# make e.g. post.uml_1-test.SL_01.solutions.txt
# from e.g.      uml_1-test.examples
#                uml_1_SL_01.wam
#               params.SL_01.wts
${DIR}/post.${NAME}-test.%.solutions.txt: ${DIR}/${NAME}-test.examples ${DIR}/${NAME}_%.wam params.%.wts
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${DIR}/${NAME}.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED} --params $(word 3,$^)

# make e.g. pre.uml_1-train.SL_01.solutions.txt
# from e.g.     uml_1-train.examples
#               uml_1_SL_01.wam
${DIR}/pre.${NAME}-train.%.solutions.txt: ${DIR}/${NAME}-train.examples ${DIR}/${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${DIR}/${NAME}-train.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED}

# make e.g. post.uml_1-train.SL_01.solutions.txt
# from e.g.      uml_1-train.examples
#                uml_1_SL_01.wam
#               params.SL_01.wts
${DIR}/post.${NAME}-train.%.solutions.txt: ${DIR}/${NAME}-train.examples ${DIR}/${NAME}_%.wam params.%.wts
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.QueryAnswerer --programFiles ${DIR}/${NAME}.cfacts:$(word 2,$^) --queries $< --solutions $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${UNNORMALIZED} --params $(word 3,$^)


# make e.g.      params.alone_01.wts
# from e.g. uml_1-train.alone_01.examples.grounded
params.%.wts: ${DIR}/${TRAIN}.%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Trainer --train $< --params $@ --threads ${THREADS} --srw ${TSRW} --epochs ${EPOCHS} --apr ${APR} ${WEIGHTINGSCHEME}

# make e.g. uml_1-train.SL_01.examples.grounded
# from e.g. uml_1-train.examples
#           uml_1_SL_01.wam
${DIR}/${NAME}-train.%.examples.grounded: ${DIR}/${NAME}-train.examples ${DIR}/${NAME}_%.wam
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.Grounder --programFiles ${DIR}/${NAME}.cfacts:$(word 2,$^) --queries $< --grounded $@ --prover ${PROVER} --threads ${THREADS} --apr ${APR} ${WEIGHTINGSCHEME}

## Additional targets for building an executable program from gradient-generated rules:

# uml_1_delta_01.ppr + h22.ppr = uml_1_SL_01.ppr
${DIR}/${NAME}_SL_%.ppr: ${DIR}/${NAME}_delta_%.ppr ${SL}
	cat $^ > $@

# uml_1_delta_01.ppr + interp.ppr = uml_1_alone_01.ppr
${DIR}/${NAME}_alone_%.ppr: ${DIR}/${NAME}_delta_%.ppr ${INTERP}
	cat $^ > $@

# Actually run the iterated structural gradient procedure, and supporting target:

isg:
	python scripts/iterativeGradientFinder.py ${DIR}/${NAME} ${ITERS}

# make e.g.           uml_1_01.gradient
# from e.g. uml_1-train.SL_01.examples.grounded
# first training for e.g.   01 epochs
${DIR}/${NAME}_%.gradient: ${DIR}/${NAME}-train.SL_%.examples.grounded
	java ${JOPTS} -cp ${CP} edu.cmu.ml.proppr.GradientFinder --grounded $< --threads ${THREADS} --apr ${APR} --epochs $* --srw ${GSRW} --gradient $@ --trainer streaming